# **Getting a taste of Research Papers-ðŸ’¯**

![alt text](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQzoY5RWPO4HIgxfyVnb4ZEUONYkBD7g6JOhxmnfjkGlKoAeZYSEQ)

**Repo to track my progress on New Resolution of reading, understanding and tinkering with Machine Learning Research Papers.**

>Sources:<br/>
* [ArXiv.org](https://arxiv.org)<br/>
* [Nurture.ai](http://nurture.ai)<br/>
* [Papers with Code](https://paperswithcode.com)<br/>
* [Zaur Fataliyev's pwc](https://github.com/zziz/pwc)<br/>
* [Twitter Bot of Miles Brudage](https://twitter.com/BrundageBot)</br>




|Category  |
|---       |
|[Computer Vision](#computer-vision)|
|[Convolutional Neural Networks](#convolutional-neural-networks)|
|[Federated Learning](#federated-learning)|
|[Generative Models](#generative-models)|
|[Geometric Deep Learning](#geometric-deep-learning) | 
|[Initialization And Optimization](#initialization-and-optimization) |
|[Miscellaneous](#miscellaneous)|
|[Natural Language Processing](#natural-language-processing)|
|[Reinforcement Learning](#reinforcement-learning)|


### Computer Vision
||Title|Tags|
|---|---|---|
|1.|[Panoptic Feature Pyramid Networks](https://arxiv.org/pdf/1901.02446.pdf)| 
|2.|[Attention Augmented Convolutional Networks](https://arxiv.org/pdf/1904.09925.pdf)
|3.|[Rethinking Atrous Convolution for Semantic Image Segmentation](https://arxiv.org/pdf/1706.05587.pdf)

### Convolutional Neural Networks
||Title |Tags|
|---|---|---|
|1.|[Bag of Tricks for Image Classification with Convolutional Neural Networks](https://arxiv.org/pdf/1812.01187.pdf)| 
|2.|[Drop an Octave: Reducing Spatial Redundancy in Convolutional Neural Networks with Octave Convolution](https://arxiv.org/pdf/1904.05049.pdf)|
|3.|[Local Relation Networks for Image Recognition](https://arxiv.org/pdf/1904.11491.pdf)|

### Federated Learning
||Title |Tags|
|---|---|---|
|1.|[Towards Federated Learning at Scale](https://arxiv.org/pdf/1902.01046.pdf)||
|2.|[Federated Learning for Mobile Keyboard Prediction](https://arxiv.org/pdf/1811.03604.pdf)|
|3.|[Federated Reinforcement Learning](https://arxiv.org/pdf/1901.08277.pdf)|

### Generative Models
||Title|Tags|
|---|---|---|
|1.|[A Style-Based Generator Architecture for Generative Adversarial Networks ](https://arxiv.org/pdf/1812.04948.pdf)|

### Geometric Deep Learning
||Title|Tags|
|---|---|---|
|1   |[Deep Learning on Graphs : A Survey](https://arxiv.org/abs/1812.04202)    |  
|2   |[Graph Neural Networks: A Review of Methods and Applications](https://arxiv.org/abs/1812.08434)    |	  
|3   |[A Comprehensive Survey on Graph Neural Networks](https://arxiv.org/abs/1901.00596)    |	 


### Initialization And Optimization
||Title|Tags|
|---|---|---|
|1.|[Fixup Initialization: Residual Learning Without Normalization](https://arxiv.org/pdf/1901.09321.pdf)|
|2.|[A disciplined approach to neural network hyper-parameters: Part 1 -- learning rate, batch size, momentum, and weight decay](https://arxiv.org/pdf/1803.09820.pdf)
|3.|[L4: Practical loss-based stepsize adaptation for deep learning](https://arxiv.org/pdf/1802.05074.pdf)|

### Miscellaneous
||Title|Tags|
|---|---|---|
|1.|[EDA: Easy Data Augmentation Techniques for Boosting Performance on Text Classification Task](https://arxiv.org/pdf/1901.11196.pdf)|
|2.|[Attentive Neural Processes](https://arxiv.org/pdf/1901.05761.pdf)|
|3.|[TensorLy: Tensor Learning in Python](https://arxiv.org/pdf/1610.09555.pdf)|
|4.|[Semantic Image Synthesis with Spatially-Adaptive Normalization](https://arxiv.org/pdf/1903.07291.pdf)
|5.|[Meta-Sim: Learning to Generate Synthetic Datasets](https://arxiv.org/pdf/1904.11621.pdf)|

### Natural Language Processing
||Title  |Tags|
|--- |---     |---|
|1.|[BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://paperswithcode.com/paper/bert-pre-training-of-deep-bidirectional2)|
|2.|[Parameter-Efficient Transfer Learning for NLP](https://arxiv.org/pdf/1902.00751.pdf)|
|3.|[Attention Is All You Need](https://arxiv.org/pdf/1706.03762.pdf)|
|4.|[Visualizing Attention in Transformer-Based Language Representation Models](https://arxiv.org/pdf/1904.02679.pdf)|
|5.|[Language Models with Transformers](https://arxiv.org/pdf/1904.09408.pdf)
|6.|[Generating Long Sequences with Sparse Transformers](https://d4mucfpksywv.cloudfront.net/Sparse_Transformer/sparse_transformers.pdf)

### Reinforcement Learning
||Title|Tags|
|---|---|---|
|1.   |[Scalable agent alignment via reward modeling: a research direction](https://arxiv.org/pdf/1811.07871) | 
|2.|[Reinforcement Learning with Attention that Works: A Self-Supervised Approach](https://arxiv.org/pdf/1904.03367.pdf)|
